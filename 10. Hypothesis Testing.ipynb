{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "Before exploring the topic of Hypothesis testing, it is important to understand some key concepts.\n",
    "\n",
    "## Population Parameters vs. Sample Statistics\n",
    "\n",
    "A **Parameter** represents a characteristic of an entire population, such as the population mean. Since it is not possible to measure the entire population, the actual value of the parameter is often unknown. The population mean and standard deviation are two commonly used parameters, represented in statistics by Greek symbols, such as $\\mu$ (mu) for the mean and $\\sigma$ (sigma) for the standard deviation.\n",
    "\n",
    "On the other hand, a **Statistic** is a characteristic of a sample that is calculated from a sample. For instance, if you collect a sample and compute its mean and standard deviation, these would be sample statistics. In statistics, latin letters are used to represent sample parameters.\n",
    "\n",
    "**Inferential statistics** involves using sample statistics to make inferences about a population. This means that we use sample statistics to estimate population parameters. In order to draw valid conclusions, it is important to use representative sampling techniques, such as random sampling, to obtain unbiased estimates. Unbiased estimates are considered correct on average, while biased estimates are systematically too high or too low.  \n",
    "\n",
    "## Parametric vs Nonparametric Analysis\n",
    "\n",
    "**Parametric statistics** is a field of statistics that operates under the assumption that the sample data being analyzed originates from populations that can be accurately described by probability distributions with a fixed set of parameters. As a result, parametric analysis is the most widely utilized statistical method.\n",
    "\n",
    "On the other hand, **nonparametric tests** don't make any assumptions about the underlying probability distribution of the data being analyzed.\n",
    "\n",
    "## Significance Level (Alpha)\n",
    "\n",
    "The **significance level** is a measure that determines the required strength of evidence from the sample data in order to conclude the presence of an effect in the population. Also known as alpha ($\\alpha$), the significance level is a predetermined evidentiary threshold that is established prior to the study. It determines the level of evidence required to reject the null hypothesis in favor of the alternative hypothesis.\n",
    "\n",
    "The significance level reflects the probability of rejecting the null hypothesis when it is actually true. In other words, it is the risk of incorrectly stating that an effect exists when there is no effect. Lower significance levels indicate a higher evidentiary threshold, as more robust evidence is required before the null hypothesis can be rejected. For example, a significance level of 0.05 indicates that there is a 5% chance of making a false positive error, or stating that an effect exists when it does not.\n",
    "\n",
    "## P-Values\n",
    "\n",
    "**P-values** are a measure of the strength of evidence against the null hypothesis provided by the sample data. If a P-value is less than the established significance level, it indicates that the results are statistically significant.\n",
    "\n",
    "The P-value is the probability of observing an effect in the sample data as extreme, or even more extreme, than the one observed if the null hypothesis were true. In simpler terms, it measures how much the sample data contradicts the null hypothesis. Lower P-values represent stronger evidence against the null hypothesis.\n",
    "\n",
    "When a P-value is less than or equal to the significance level, the null hypothesis is rejected and the results are considered statistically significant. This means that the sample data provides sufficient evidence to support the alternative hypothesis that the effect exists in the population.\n",
    "\n",
    "On the other hand, when a P-value is greater than the significance level, the sample data do not provide enough evidence to conclude that the effect exists, and the null hypothesis is not rejected.\n",
    "\n",
    "In statistical terms, these decisions are expressed as follows:\n",
    "\n",
    "+ Reject the null hypothesis when the P-value is less than or equal to the significance level.\n",
    "+ Fail to reject the null hypothesis when the P-value is greater than the significance level.\n",
    "\n",
    "## Hypothesis Testing\n",
    "**Hypothesis testing** is a statistical technique that evaluates the evidence of two opposing statements (hypotheses) about a population based on sample data. These hypotheses are known as the null hypothesis and the alternative hypothesis.\n",
    "\n",
    "The objective of hypothesis testing is to assess the sample statistic and its corresponding sampling error to determine which of the two hypotheses is more strongly supported by the data. If the null hypothesis can be rejected, it means that the results are statistically significant and the alternative hypothesis is favored, suggesting that an effect exists in the population.\n",
    "\n",
    "It is important to note that failing to reject the null hypothesis does not necessarily mean that the null hypothesis is true, nor does rejecting the null hypothesis necessarily imply that the alternative hypothesis is true. The results of a hypothesis test are only a suggestion or indication about the population, not a conclusive proof of either hypothesis.\n",
    "\n",
    "The null hypothesis is the theory that there is no effect (i.e., the effect size is equal to zero). It is commonly represented by $H_0$.\n",
    "\n",
    "The alternative hypothesis is the opposite theory, stating that the population parameter does not equal the value specified in the null hypothesis (i.e., there is a non-zero effect). It is usually represented by $H_1$ or $H_A$.\n",
    "\n",
    "The steps involved in hypothesis testing are as follows:\n",
    "\n",
    "1. State the null and alternative hypothesis.\n",
    "2. Specify the significance level and calculate the critical value of the test statistic.\n",
    "3. Choose the appropriate test based on factors such as the number of samples, population distribution, statistic being tested, sample size, and knowledge of the population standard deviation.\n",
    "4. Calculate the relevant test statistic (z-statistic, t-statistic, chi-square statistic, or f-statistic) or p-value.\n",
    "5. Compare the calculated test statistic with the critical test statistic or the p-value with the significance level.\n",
    "    - If using the test statistic:\n",
    "        - Reject the null hypothesis if the calculated test statistic is greater than the critical test statistic (upper-tail test)\n",
    "        - Reject the null hypothesis if the calculated test statistic is less than the critical test statistic (lower-tail test)\n",
    "    - If using the p-value:\n",
    "        - Reject the null hypothesis if the p-value is less than the significance level.\n",
    "6. Draw a conclusion based on the comparison made in step 5.\n",
    "\n",
    "## **Confidence interval**\n",
    "\n",
    "A **confidence interval** can be calculated for various parameters such as population mean, population proportion, difference of population means, and difference of population proportions, among others. To construct a confidence interval, one needs to have a sample statistic that estimates the population parameter of interest and a measure of variability or standard error for that statistic. The confidence interval is calculated by adding and subtracting the standard error to the sample statistic. The result is a range of values that contains the true population parameter with a specified level of confidence.\n",
    "\n",
    "The key concept behind a confidence interval is that if we repeated our sampling process many times, the true population parameter would fall within the confidence interval for the specified percentage of these samples. For example, if we have a 95% confidence interval for a population mean, we can say that if we repeated our sampling process 100 times, the true population mean would fall within the calculated confidence interval 95 times out of 100.\n",
    "\n",
    "In conclusion, confidence intervals provide a range of plausible values for a population parameter based on the observed sample data and the level of confidence specified by the researcher. The confidence interval reflects the precision of the estimate, with wider intervals indicating less precision and narrower intervals indicating more precision.\n",
    "\n",
    "## Sampling error\n",
    "\n",
    "A **sampling error** refers to the discrepancy between a population parameter and a sample statistic. In a study, the sampling error represents the difference between the mean calculated from a sample and the actual mean of the population. Despite using a random sample selection process, sampling errors are still a possibility as the sample may not perfectly reflect the population with regards to numerical values such as means and standard deviations. To improve the accuracy of generalizing findings from a sample to a population, it's essential to minimize the sampling error. One way to do this is to increase the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
