{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e53dce",
   "metadata": {},
   "source": [
    "## Effect Size\n",
    "\n",
    "\n",
    "An effect size is a way to quantify the difference between two groups. While a p-value can tell us whether or not there is a statistically significant difference between two groups, an effect size can tell us how large this difference actually is. Statistical significance alone can be misleading because it's influenced by the sample size. Increasing the sample size always makes it more likely to find a statistically significant effect, no matter how small the effect truly is in the real world.\n",
    "In practice, effect sizes are much more interesting and useful to know than p-values.\n",
    "\n",
    "There are different measures for effect sizes. The most common effect sizes are Cohen's d and Pearson's r.   \n",
    "\n",
    "Cohen's d measures the size of the difference between two groups while Pearson's r measures the strength of the relationship between two variables.\n",
    "\n",
    "### Cohen's d -  Standardized Mean Difference\n",
    "Cohen's d is designed for comparing two groups. It takes the difference between two means and expresses it in standard deviation units. It tells you how many standard deviations lie between the two means.\n",
    "\n",
    "$$ d =\\frac{ \\overline x_1 - \\overline x_2 }{S}$$\n",
    "\n",
    "where  $\\overline x_1$ and $\\overline x_2$ are mean of group 1 and group 2 respectively. $S$ is standard deviation.\n",
    "\n",
    "The choice of standard deviation in the equation depends on your research design.\n",
    "We can use:\n",
    "+  pooled standard deviation that is based on data from both groups,\n",
    "+ standard deviation from a control group.\n",
    "+ the standard deviation from the pretest data or posttest.\n",
    "\n",
    "### Pearson's r - Correlation Coefficient\n",
    "\n",
    "Pearson's $r$, or the correlation coefficient, measures the extent of a linear relationship between two variables.\n",
    "\n",
    "The formula is rather complex, so it’s best to use a statistical software to calculate Pearson's r accurately from the raw data.\n",
    "\n",
    "$$ r_{xy} = \\frac{n\\sum x_i y_i -\\sum x_i \\sum y_i}{\\sqrt{n\\sum x_i^2-(\\sum x_i)^2}{\\sqrt{n\\sum y_i^2-(\\sum y_i)^2}}}$$\n",
    "\n",
    "The main idea of the formula is to compute how much of the variability of one variable is determined by the variability of the other variable. Pearson's r is a standardized scale to measure correlations between variables that makes it unit-free. You can directly compare the strengths of all correlations with each other.\n",
    "\n",
    "### Interpreting Values\n",
    "\n",
    "+ Cohen's $d$ can take on any number between 0 and infinity, In general the greater the Cohen's d, the larger the effect size\n",
    "+ Pearson's $r$ ranges between -1 and 1. The closer the value is to 0, the smaller the effect size. A value closer to -1 or 1 indicates a higher effect size.\n",
    "\n",
    "General Rule of thumb to quantify whether an effect size is small, medium or large:\n",
    "\n",
    "**Cohen’s D:**\n",
    "\n",
    "+ A d of 0.2 or smaller is considered to be a small effect size.\n",
    "+ A d of 0.5 is considered to be a medium effect size.\n",
    "+ A d of 0.8 or larger is considered to be a large effect size.\n",
    "\n",
    "\n",
    "**Pearson Correlation Coefficient:**\n",
    "\n",
    "+ An absolute value of r around 0.1 is considered a low effect size.\n",
    "+ An absolute value of r around 0.3 is considered a medium effect size.\n",
    "+ An absolute value of r greater than .5 is considered to be a large effect size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f53b71",
   "metadata": {},
   "source": [
    "# Statistical Power\n",
    "\n",
    "Statistical power, or sensitivity, is the likelihood of a significance test detecting an effect when there actually is one.  In other words, Power is the probability that we will correctly reject the  Null Hypothesis.\n",
    "\n",
    "Lets look at an example to understand this concept. Suppose we have two distributions with minimum overlap like in the first picture below and If we collect small set of samples from both Green and Red distribution and compare their means using hypthesis testing. we get a small p-value say 0.0004. This causes us to correctly reject the Null hypothesis that both the sample set came from same distribution. In other words if the blue distribution says that all data points came from me, we would reject that hypothesis.  \n",
    "\n",
    "If we keep repeating this experiment a bunch of times, theres a high probability that each statistical test will correctly give us a small p-value. In other words, there is high probabily that the null hypothesis that all the data came from the same distribution will be correctly rejected.\n",
    "\n",
    "But once in a while we will get a trial like in the second picture below, where the two sample sets looks like they came from same distribution due to overlapping sample points and we get high p-value like 0.08. That means even though we know that data came from two different distributions we cannot correctly reject the null hypothesis that all the data came from the same distribution. Since these two distributions are far apart and very littel overlap the probabily of correctly rejecting the null hypothesis is high. Since power is the probability that we will correctly reject the null hypothesis. In this example We have large amount of Power.\n",
    "\n",
    "![](data/p1.png)\n",
    "\n",
    "![](data/p2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f028e681",
   "metadata": {},
   "source": [
    "Now lets look at different case say if we have large over lap in the distributions as shown in the first picture below. Most of the time when we compare means of two distribution we get a high p-vale and we fail to reject the null hypothesis that the data comes from the same distribution. But again once in a while when the sample data points are from far extremes of the distribution like in the second picture below. we get a small p-value and we will correctly able to reject the null hypothesis that the data comes from the same distribution. Due to the over lap the probability of correctly rejecting the hypothesis is low, we have relatively low power.\n",
    "\n",
    "The good news is we can increase they power by increasing the number of samples we collect. The power Analysis will tell us how many measurements we need to collect to have a good amount of power.\n",
    "\n",
    "![](data/p3.png)\n",
    "\n",
    "![](data/p4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a25a86",
   "metadata": {},
   "source": [
    "Before we learn how to do power analysis. Lets understand why do we need to perform power analysis in detail. \n",
    "\n",
    "### What is the need for Power Analysis?\n",
    "\n",
    "In hypothesis testing, we start with a null hypothesis of no effect and an alternative hypothesis of a true effect. The goal is to collect enough data from a sample to statistically test whether we can reasonably reject the null hypothesis in favor of the alternative hypothesis. In doing so, there's always a risk of making one of two decision errors when interpreting study results:  \n",
    "\n",
    "**Type I error**: Rejecting the null hypothesis of no effect when it is actually true.  \n",
    "**Type II error**: Not rejecting the null hypothesis of no effect when it is actually false.  \n",
    "\n",
    "Power is the probability of avoiding a Type II error. The higher the statistical power of a test, the lower the risk of making a Type II error.  Power is usually set at 80%. This means that if there are true effects to be found in 100 different studies with 80% power, only 80 out of 100 statistical tests will actually detect them. If we don't ensure sufficient power, our study may not be able to detect a true effect at all. This means that resources like time and money are wasted, and it may even be unethical to collect data from participants.  On the flip side, too much power means our tests are highly sensitive to true effects, including very small ones. This may lead to finding statistically significant results with very little usefulness in the real world.  To balance these pros and cons of low versus high statistical power, we should use a **Power Analysis** to set an appropriate level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf82cb",
   "metadata": {},
   "source": [
    "# Power Analysis\n",
    "\n",
    "Power is mainly influenced by sample size, effect size, and significance level. A power analysis can be used to determine the necessary sample size for a study. Having enough statistical power is necessary to draw accurate conclusions about a population using sample data.\n",
    "\n",
    "Power is effected by several things however there are two main factors listed below:\n",
    "\n",
    "+ How much **over lap** is there between the two distributions we want to identify with our study.\n",
    "+ The **sample size** the number of samples we collect from each group\n",
    "\n",
    "If we want Power to be 80% and if there is very little overlap, a small sample size will do. However, if the over overlap is more between the two distributions we need larger sample size in order to have 80% power.\n",
    "\n",
    "To understand the **Relationship between Overlap and Sample size**, the first thing we need to realize is that when we do a statistical test we usualy don't compare individual measurements instead we compare the sample means of the data. so lets see what happens when we calcuate means with different sample sizes.\n",
    "\n",
    "If sample size is less then we see there is lot of variation in estimated means for a distribution and all that variation makes it hard to be confident that any single estimated mean is good estimate of the population mean and there is over lap between the estimated means of the two distributions.\n",
    "But if the sample size is more, then the estimated means are so close to the population mean that they no longer overlap. And that suggests that there is high probability that we correctly reject the null hypothesis that both samples came from the same distributution. If the sample size is large we can have high power. Also he central limit theorem states that these results apply to any type of distribution.\n",
    "\n",
    "\n",
    "A power analysis is made up of four main components. If you know or have estimates for any three of these, we can calculate the fourth component.\n",
    "\n",
    "**Statistical power:** the likelihood that a test will detect an effect of a certain size if there is one, usually set at 80% or higher.\n",
    "\n",
    "**Sample size:** the minimum number of observations needed to observe an effect of a certain size with a given power level.\n",
    "\n",
    "**Significance level (alpha):** the maximum risk of rejecting a true null hypothesis that you are willing to take, usually set at 5%.\n",
    "\n",
    "**Expected effect size:** The combined effect of s.d and means of two distributions due to overlap are caputured by Effect size(d). There are many different ways to capture the effect.\n",
    "\n",
    "Before starting a study, we can use a power analysis to calculate the minimum sample size for a desired power level and significance level and an expected effect size.Traditionally, the significance level is set to 5% and the desired power level to 80%. That means we only need to figure out an expected effect size to calculate a sample size from a power analysis.\n",
    "\n",
    "The ``stats.power`` module of the statsmodels package in Python contains the required functions for carrying out power analysis for the most commonly used statistical tests such as t-test, normal based test, F-tests, and Chi-square goodness of fit test. It’s solve_power function takes 3 of the 4 components mentioned above as input parameters and calculates the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5020de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
