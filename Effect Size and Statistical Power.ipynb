{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e53dce",
   "metadata": {},
   "source": [
    "## Effect Size\n",
    "\n",
    "Effect size tells you how meaningful the relationship between variables or the difference between groups is. It indicates the practical significance of a research outcome. A large effect size means that a research finding has practical significance, while a small effect size indicates limited practical applications.\n",
    "\n",
    "Statistical significance alone can be misleading because it's influenced by the sample size. Increasing the sample size always makes it more likely to find a statistically significant effect, no matter how small the effect truly is in the real world.\n",
    "\n",
    "There are different measures for effect sizes. The most common effect sizes are Cohen's d and Pearson's r.  Cohen's d measures the size of the difference between two groups while Pearson's r measures the strength of the relationship between two variables.\n",
    "\n",
    "### Cohen's d\n",
    "Cohen's d is designed for comparing two groups. It takes the difference between two means and expresses it in standard deviation units. It tells you how many standard deviations lie between the two means.\n",
    "\n",
    "$$ d =\\frac{ \\overline x_1 - \\overline x_2 }{S}$$\n",
    "\n",
    "where  $\\overline x_1$ and $\\overline x_2$ are mean of group 1 and group 2 respectively. S is standard deviation.\n",
    "\n",
    "The choice of standard deviation in the equation depends on your research design.\n",
    "We can use:\n",
    "+  pooled standard deviation that is based on data from both groups,\n",
    "+ standard deviation from a control group.\n",
    "+ the standard deviation from the pretest data or posttest.\n",
    "\n",
    "### Pearson's r\n",
    "Pearson's r, or the correlation coefficient, measures the extent of a linear relationship between two variables.\n",
    "\n",
    "The formula is rather complex, so it’s best to use a statistical software to calculate Pearson's r accurately from the raw data.\n",
    "\n",
    "$$ r_{xy} = \\frac{n\\sum x_i y_i -\\sum x_i \\sum y_i}{\\sqrt{n\\sum x_i^2-(\\sum x_i)^2}{\\sqrt{n\\sum y_i^2-(\\sum y_i)^2}}}$$\n",
    "\n",
    "The main idea of the formula is to compute how much of the variability of one variable is determined by the variability of the other variable.\n",
    "\n",
    "Pearson's r is a standardized scale to measure correlations between variables—that makes it unit-free. You can directly compare the strengths of all correlations with each other.\n",
    "\n",
    "Cohen's d can take on any number between 0 and infinity, while Pearson's r ranges between -1 and 1.\n",
    "\n",
    "In general, the greater the Cohen's d, the larger the effect size. For Pearson's r, the closer the value is to 0, the smaller the effect size. A value closer to -1 or 1 indicates a higher effect size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f53b71",
   "metadata": {},
   "source": [
    "# Statistical Power\n",
    "\n",
    "Statistical power, or sensitivity, is the likelihood of a significance test detecting an effect when there actually is one.\n",
    "\n",
    "A true effect is a real, non-zero relationship between variables in a population. An effect is usually indicated by a real difference between groups or a correlation between variables.\n",
    "\n",
    "High power in a study indicates a large chance of a test detecting a true effect. Low power means that your test only has a small chance of detecting a true effect or that the results are likely to be distorted by random and systematic error.\n",
    "\n",
    "Power is mainly influenced by sample size, effect size, and significance level. A power analysis can be used to determine the necessary sample size for a study.\n",
    "\n",
    "Having enough statistical power is necessary to draw accurate conclusions about a population using sample data.\n",
    "\n",
    "In hypothesis testing, you start with a null hypothesis of no effect and an alternative hypothesis of a true effect (your actual research prediction).\n",
    "\n",
    "The goal is to collect enough data from a sample to statistically test whether you can reasonably reject the null hypothesis in favor of the alternative hypothesis.\n",
    "\n",
    "There’s always a risk of making one of two decision errors when interpreting study results:\n",
    "\n",
    "Type I error: rejecting the null hypothesis of no effect when it is actually true.\n",
    "Type II error: not rejecting the null hypothesis of no effect when it is actually false.\n",
    "\n",
    "Power is the probability of avoiding a Type II error. The higher the statistical power of a test, the lower the risk of making a Type II error.\n",
    "\n",
    "Power is usually set at 80%. This means that if there are true effects to be found in 100 different studies with 80% power, only 80 out of 100 statistical tests will actually detect them.\n",
    "\n",
    "If you don’t ensure sufficient power, your study may not be able to detect a true effect at all. This means that resources like time and money are wasted, and it may even be unethical to collect data from participants (especially in clinical trials).\n",
    "\n",
    "On the flip side, too much power means your tests are highly sensitive to true effects, including very small ones. This may lead to finding statistically significant results with very little usefulness in the real world.\n",
    "\n",
    "To balance these pros and cons of low versus high statistical power, you should use a power analysis to set an appropriate level.\n",
    "\n",
    "A power analysis is a calculation that helps you determine a minimum sample size for your study.\n",
    "\n",
    "A power analysis is made up of four main components. If you know or have estimates for any three of these, you can calculate the fourth component.\n",
    "\n",
    "**Statistical power:** the likelihood that a test will detect an effect of a certain size if there is one, usually set at 80% or higher.\n",
    "\n",
    "**Sample size:** the minimum number of observations needed to observe an effect of a certain size with a given power level.\n",
    "\n",
    "**Significance level (alpha):** the maximum risk of rejecting a true null hypothesis that you are willing to take, usually set at 5%.\n",
    "\n",
    "**Expected effect size:** a standardized way of expressing the magnitude of the expected result of your study, usually based on similar studies or a pilot study.  \n",
    "Before starting a study, you can use a power analysis to calculate the minimum sample size for a desired power level and significance level and an expected effect size.\n",
    "\n",
    "Traditionally, the significance level is set to 5% and the desired power level to 80%. That means you only need to figure out an expected effect size to calculate a sample size from a power analysis.\n",
    "\n",
    "Sample size\n",
    "Sample size is positively related to power. A small sample (less than 30 units) may only have low power while a large sample has high power.\n",
    "\n",
    "Increasing the sample size enhances power, but only up to a point. When you have a large enough sample, every observation that’s added to the sample only marginally increases power. This means that collecting more data will increase the time, costs and efforts of your study without yielding much more benefit.\n",
    "\n",
    "Your research design is also related to power and sample size:\n",
    "\n",
    "In a within-subjects design, each participant is tested in all treatments of a study, so individual differences will not unevenly affect the outcomes of different treatments.\n",
    "In a between-subjects design, each participant only takes part in a single treatment, so with different participants in each treatment, there is a chance that individual differences can affect the results.\n",
    "A within-subjects design is more powerful, so fewer participants are needed. More participants are needed in a between-subjects design to establish relationships between variables.\n",
    "\n",
    "Significance level\n",
    "The significance level of a study is the Type I error probability, and it’s usually set at 5%. This means your findings have to have a less than 5% chance of occurring under the null hypothesis to be considered statistically significant.\n",
    "\n",
    "Significance level is correlated with power: increasing the significance level (e.g., from 5% to 10%) increases power. When you decrease the significance level, your significance test becomes more conservative and less sensitive to detecting true effects.\n",
    "\n",
    "Researchers have to balance the risks of committing Type I and II errors by considering the amount of risk they’re willing to take in making a false positive versus a false negative conclusion.\n",
    "\n",
    "Effect size\n",
    "Effect size is the magnitude of a difference between groups or a relationship between variables. It indicates the practical significance of a finding.\n",
    "\n",
    "While high-powered studies can help you detect medium and large effects in studies, low-powered studies may only catch large ones.\n",
    "\n",
    "There’s always some sampling error involved when using data from samples to make inferences about populations. This means there’s always a discrepancy between the observed effect size and the true effect size. Effect sizes in a study can vary due to random factors, measurement error, or natural variation in the sample.\n",
    "\n",
    "Low-powered studies will mostly detect true effects only when they are large in a study. That means that, in a low-powered study, any observed effect is more likely to be boosted by unrelated factors.\n",
    "\n",
    "If low-powered studies are the norm in a particular field, such as neuroscience, the observed effect sizes will consistently exaggerate or overestimate true effects.\n",
    "\n",
    "Aside from the four major components, other factors need to be taken into account when determining power.\n",
    "\n",
    "Variability\n",
    "The variability of the population characteristics affects the power of your test. High population variance reduces power.\n",
    "\n",
    "In other words, using a population that takes on a large range of values for a variable will lower the sensitivity of your test, while using a population where the variable is relatively narrowly distributed will heighten the sensitivity of the test.\n",
    "\n",
    "Using a fairly specific population with defined demographic characteristics can lower the spread of the variable of interest and improve power.\n",
    "\n",
    "Measurement error\n",
    "Measurement error is the difference between the true value and the observed or recorded value of something. Measurements can only be as precise as the instruments and researchers that measure them, so some error is almost always present.\n",
    "\n",
    "The higher the measurement error in a study, the lower the statistical power of a test. Measurement error can be random or systematic:\n",
    "\n",
    "Random errors are unpredictable and unevenly alter measurements due to chance factors (e.g., mood changes can influence survey responses, or having a bad day may lead to researchers misrecording observations).\n",
    "Systematic errors affect data in predictable ways from one measurement to the next (e.g., an incorrectly calibrated device will consistently record inaccurate data, or problematic survey questions may lead to biased responses).\n",
    "\n",
    "Since many research aspects directly or indirectly influence power, there are various ways to improve power. While some of these can usually be implemented, others are costly or involve a tradeoff with other important considerations.\n",
    "\n",
    "Increase the effect size. To increase the expected effect in an experiment, you could manipulate your independent variable more widely (e.g., spending 1 hour instead of 10 minutes in nature) to increase the effect on the dependent variable (stress level). This may not always be possible because there are limits to how much the outcomes in an experiment may vary.\n",
    "\n",
    "Increase sample size. Based on sample size calculations, you may have room to increase your sample size while still meaningfully improving power. But there is a point at which increasing your sample size may not yield high enough benefits.\n",
    "\n",
    "Increase the significance level. While this makes a test more sensitive to detecting true effects, it also increases the risk of making a Type I error.\n",
    "\n",
    "Reduce measurement error. Increasing the precision and accuracy of your measurement devices and procedures reduces variability, improving reliability and power.  Using multiple measures or methods, known as triangulation, can also help reduce systematic bias.\n",
    "\n",
    "Use a one-tailed test instead of a two-tailed test. When using a t test or z tests, a one-tailed test has higher power. However, a one-tailed test should only be used when there’s a strong reason to expect an effect in a specific direction (e.g., one mean score will be higher than the other), because it won’t be able to detect an effect in the other direction. In contrast, a two-tailed test is able to detect an effect in either direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5eff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
